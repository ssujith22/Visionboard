{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d40ddff5-6722-4952-8647-9d6f747fa19a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "PySpark Coding Challenge: Analyzing Online Store Orders\n",
    "\n",
    "Task: You have a dataset containing information about orders from an online store. Your task is to use PySpark to analyze the data and answer a few questions using aggregate functions.\n",
    "\n",
    "Dataset: The dataset is in CSV format and contains the following columns: order_id, customer_id, order_date, total_amount.\n",
    "\n",
    "Questions:\n",
    "\n",
    "Calculate the total revenue generated from all orders.\n",
    "\n",
    "Find the average order amount.\n",
    "\n",
    "Identify the highest total order amount and its corresponding customer.\n",
    "\n",
    "Calculate the total number of orders for each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23376ba5-65c3-4190-bb62-6d4764edb007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Revenue: 1120\nAverage Order Amount: 186.66666666666666\nHighest Order Amount: 300\nCustomer ID: C103\n+-----------+------------+\n|customer_id|total_orders|\n+-----------+------------+\n|       C101|           3|\n|       C102|           2|\n|       C103|           1|\n+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"OnlineStoreAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"C101\", \"2023-07-01\", 150),\n",
    "    (2, \"C102\", \"2023-07-02\", 200),\n",
    "    (3, \"C101\", \"2023-07-02\", 100),\n",
    "    (4, \"C103\", \"2023-07-03\", 300),\n",
    "    (5, \"C102\", \"2023-07-04\", 250),\n",
    "    (6, \"C101\", \"2023-07-05\", 120)\n",
    "]\n",
    "columns = [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Question 1: Calculate total revenue\n",
    "total_revenue = df.select(sum(\"total_amount\")).collect()[0][0]\n",
    "print(\"Total Revenue:\", total_revenue)\n",
    "\n",
    "# Question 2: Average order amount\n",
    "average_order_amount = df.agg(avg(\"total_amount\")).collect()[0][0]\n",
    "print(\"Average Order Amount:\", average_order_amount)\n",
    "\n",
    "# Question 3: Highest total order amount and corresponding customer\n",
    "highest_order = df.orderBy(col(\"total_amount\").desc()).limit(1).first()\n",
    "print(\"Highest Order Amount:\", highest_order[\"total_amount\"])\n",
    "print(\"Customer ID:\", highest_order[\"customer_id\"])\n",
    "\n",
    "# Question 4: Total number of orders per customer\n",
    "total_orders_per_customer = df.groupBy(\"customer_id\").agg(count(\"order_id\").alias(\"total_orders\"))\n",
    "total_orders_per_customer.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark interview questions day5",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
