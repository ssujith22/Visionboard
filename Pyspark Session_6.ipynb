{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "521b3a9e-df5c-4f55-915f-a60dc4e656e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####When we create a dataframe, what will be the number of patitions at the initial stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91d6702a-2831-4789-bea7-75a87dc22b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a23d2be6-8b68-430a-8905-0ef0de35c5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####spark.conf.get(\"spark.sql.files.maxPartitionBytes)::128MB\n",
    "\n",
    "####df.rdd.getNumPartitions\n",
    "\n",
    "- decide in a way so that all the resources  should be used\n",
    "- if 9 cores are there, then number of paralleism can not be greater then 9\n",
    "- suppose we haev 1152 MB file\n",
    "- requested for 3 executors with 3 cpu cores\n",
    "- number of  total cores = 9\n",
    "- number of pattiotns = total file size/maxpartition =  1152/128 = 9 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6999c96b-a548-4d06-b38a-60f06c4800d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.config(\"spark.dynamicAllocation.enabled\", False). \\\n",
    "    config(\"spark.executors.cores\", 3). \\\n",
    "        config(\"spark.executors.instances\", 3). \\`\n",
    "        config(\"spark.executor.memory\", \"2g\"). \\\n",
    "        .master(\"local[1]\")\n",
    "      .appName(\"SparkByExamples.com\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee509c0d-c847-423c-a453-5b9d9cacfb43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#####How to decide number of executors, number of cores and executor memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24a6ef30-25af-45e6-b572-5c61fbec0561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#####scenario 1 : 16 modes and each node has 8 cores and 32 GB RAM\n",
    "#####datset : 1TB\n",
    "\n",
    "1. An executor is a Spark process responsible for executing tasks on a specific node in the cluster. Each executor is assigned a fixed number of cores and a certain amount of memory. The number of executors determines the level of parallelism at which Spark can process data.\n",
    "- Having more executors allows for better parallelism and resource utilization.\n",
    "- Each executor can work on a subset of data independently, which can lead to increased processing speed.\n",
    "- However, it’s important to strike a balance between the number of executors and the available cluster resources. If the number of executors is too high, it can lead to excessive memory usage and increased overhead due to task scheduling.\n",
    "\n",
    "2. Spark Cores\n",
    "The number of cores refers to the total number of processing units available on the machines in your Spark cluster. It represents the parallelism level at which Spark can execute tasks. Each core can handle one concurrent task.\n",
    "\n",
    "- Increasing the number of cores allows,\n",
    "\n",
    "- Spark to execute more tasks simultaneously, which can improve the overall throughput of your application.\n",
    "- However, adding too many cores can also introduce overhead due to task scheduling and inter-node communication, especially if the cluster resources are limited.\n",
    "\n",
    "3. Configuring Spark Number of Executors and its Cores\n",
    "Configuring the number of cores and executors in Apache Spark depends on several factors, including\n",
    "\n",
    "- The characteristics of your workload,\n",
    "- The available cluster resources, and\n",
    "Specific requirements of your application.\n",
    "\n",
    "**Number of executors:\n",
    "**The number of executors should be equal to the number of cores on each node in the cluster.\n",
    "If there are more cores than nodes, then the number of executors should be equal to the number of nodes.\n",
    "**Memory per executor:\n",
    "**The amount of memory allocated to each executor should be based on the size of the data that will be processed by that executor.\n",
    "It is important to leave some memory available for the operating system and other processes.\n",
    "A good starting point is to allocate 1GB of memory per executor.\n",
    "**Number of partitions:\n",
    "**The number of partitions used for shuffle operations should be equal to the number of executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c15a2aab-4f6d-4772-b522-39edeff5b67a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "1. Available Resources:\n",
    "Total cores in the cluster = 16 nodes * 8 cores per node = 128 cores\n",
    "Total memory in the cluster = 16 nodes * 32 GB per node = 512 GB\n",
    "2. Workload Characteristics: \n",
    "Large dataset size and complex computations suggest that you need a high level of parallelism to efficiently process the data. Let’s assume that you want to allocate 80% of the available resources to Spark.\n",
    "\n",
    "####Tiny Executor Configuration\n",
    "\n",
    "Executor Memory and Cores per Executor: Considering having 1 core per executor,\n",
    "* Number of executors per node=8,\n",
    "* Executor-memory=32/8=4GB\n",
    "Calculating the Number of Executors: To calculate the number of executors, divide the available memory by the executor memory:\n",
    "* Total memory available for Spark = 80% of 512 GB = 410 GB\n",
    "* Number of executors = Total memory available for Spark / Executor memory = 410 GB / 4 GB ≈ 102 executors\n",
    "* Number of executors per node = Total Number of Executors/ Number of Nodes = 102/16 ≈ 6 Executors/Node\n",
    "\n",
    "- So, in this example, you would configure Spark with 102 executors, each executor having 1 core and 4 GB of memory.\n",
    "\n",
    "####Fat Executor Configuration\n",
    "\n",
    "\n",
    "The other way of configuring Spark Executor and its core is setting the maximum utility configuration i.e. having only one Executor per node and optimizing it based on the application performance.\n",
    "\n",
    "Executor Memory and Cores per Executor: Considering having 8 cores per executor,\n",
    "* Number of executors per node= number of cores for a node/ number of cores for an executor = 8/8 = 1,\n",
    "* Executor-memory=32/1= 32GB\n",
    "Calculating the Number of Executors: To calculate the number of executors, divide the available memory by the executor memory:\n",
    "* Total memory available for Spark = 80% of 512 GB = 410 GB\n",
    "* Number of executors = Total memory available for Spark / Executor memory = 410 GB / 32 GB ≈ 12 executors\n",
    "* Number of executors per node = Total Number of Executors/ Number of Nodes = 12/16 ≈ 1 Executors/Node\n",
    "\n",
    "\n",
    "**So, in this example, we would configure Spark with 16 executors, each executor having 8 core and 32 GB of memory.####**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ffb33a1-f446-42cf-a75c-516ee45f53b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Balanced Executor Configuration\n",
    "\n",
    " **Spark founder Databricks after several trail and error testing the spark Executor and cores configuration, they recommends to have 2-5 cores per executor as the best initial efficient configuration for running the application smoothly.**\n",
    "\n",
    "Executor Memory and Cores per Executor: Considering having 3 cores per executor, Leaving 1 core per node for daemon processes\n",
    "* Number of executors per node= (number of cores for a node – core for daemon process)/ number of cores for an executor = 7/3 ≈ 2,\n",
    "* Executor-memory=Total memory per node/ number executors per node = 32/2= 16GB\n",
    "Calculating the Number of Executors: To calculate the number of executors, divide the available memory by the executor memory:\n",
    "* Total memory available for Spark = 80% of 512 GB = 410 GB\n",
    "* Number of executors = Total memory available for Spark / Executor memory = 410 GB / 16 GB ≈ 32 executors\n",
    "* Number of executors per node = Total Number of Executors/ Number of Nodes = 32/16 = 2 Executors/Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a497fca5-7446-4913-baa2-e76e382ad599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Tiny Executor Configuration:**\n",
    "\n",
    "- Pros: Resource efficiency, increased task isolation, better granularity for small tasks, and suitability for limited resources or a high number of concurrent tasks.\n",
    "- Cons: Increased overhead, limited parallelism, potential bottlenecks, and memory overhead.\n",
    "\n",
    "**Fat Executor Configuration:**\n",
    "- Pros: Increased parallelism, reduced overhead, enhanced data locality, and improved performance for complex tasks.\n",
    "- Cons: Resource overallocation, reduced task isolation, longer startup times, and potential challenges in resource sharing.\n",
    "\n",
    "**Balanced Executor Configuration:**\n",
    "- Pros: Optimal resource utilization, reasonable parallelism, flexibility for multiple workloads, and reduced overhead.\n",
    "- Cons: Limited scaling, trade-off in task isolation, potential task granularity issues, and complexity in resource management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4761a044-b24a-4b5e-82aa-288fcfe0c344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#####Q1 How to apply row_number on a dataframe column?\n",
    "#####Q2 Create a custom schema having columns as salary, firstname and lastname\n",
    "#####Q3 cast a column into timestamp\n",
    "#####Q4 how to drop null/how to handle nulls/how to drop na values\n",
    "#####Q5 I have multiline in one column of my dataframe, how to achiev the right values or mul;tiple line in one single column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0231ec6b-8c9f-42fc-afef-62eb4b814aa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "############df=df.withColumn(\"Salried_row_num\", row_number().over(Window.orderBy(\"Salary\")))\n",
    "\n",
    "2024-23-12 12:12:45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af9b1263-dad1-4f54-9741-fd1e0f9c0c8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c19d83b5-e703-4658-89b8-7210de304d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "(sc\n",
    "    .parallelize([Row(dt='2016_08_21 11_31_08')])\n",
    "    .toDF()\n",
    "    .withColumn(\"parsed\", to_timestamp(\"dt\", \"YYYY-MM-DD HH:mm:ss\"))\n",
    "    .show(1, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "939ae1ac-853d-46b3-8037-ecdd7ecf190c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####How can you cache data in PySpark to improve performance?\n",
    "\n",
    "e methods .cache() or .persist() allows us to store the data in memory or at the specified storage level. This task improves performance by avoiding repeated computations and reducing the need for data serialization and deserialization. \n",
    "\n",
    "# How to cache data in memory \n",
    "df_from_csv.cache()\n",
    "\n",
    "# How to persist data in local disk \n",
    "df_from_csv.persist(storageLevel=StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7333bd40-7dfe-4f23-a8f1-a00a4b3983f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####What are the key differences between RDDs, DataFrames, and Datasets in PySpark?\n",
    "1. Spark Resilient Distributed Datasets (RDD), DataFrame, and Datasets are key abstractions in Spark that enable us to work with structured data in a distributed computing environment. Even though they are all ways of representing data, they have key differences:\n",
    "\n",
    "2. RDDs are low-level APIs that lack a schema and offer control over the data. They are immutable collections of objects \n",
    "DataFrames are high-level APIs built on top of RDDs optimized for performance but are not safe-type. They organize structured and semi-structured data into named columns.\n",
    "3. Datasets combine the benefits of RDDs and DataFrames. They are high-level APIs that provide safe-type abstraction. They support Python and Scala and provide compile-time type checking while being faster than DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3590e5a5-ed68-443e-beea-1568f1ead0a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#####What is the purpose of checkpoints in PySpark?\n",
    "In PySpark, checkpointing implies that RDDs are saved to disk so this intermediate point can be referenced in the future instead of recomputing the RDD for the original source. Checkpoints provide a way to recover from failures because the driver is restarted with this previously computed state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5291fdc7-c0f6-4f27-a4e3-24f2f2c597ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#####What challenges have you faced when working with large datasets in PySpark? How did you overcome them?\n",
    "With this question, we can relate to our own experience and tell a particular case in which encountered challenges with PySpark and large datasets that can include some of the following:\n",
    "\n",
    "- Memory management and resource utilization.\n",
    "- Data skewness and uneven workload distribution.\n",
    "- Performance optimization, especially for wide transformations and shuffles.\n",
    "- Debugging and troubleshooting complex job failures.\n",
    "- Efficient data partitioning and storage\n",
    ".\n",
    "To overcome these issues, PySpark provides partitioning of the dataset, caching intermediate results, using built-in optimization techniques, robust cluster management, and leveraging fault tolerance mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56641b21-7138-4569-9b8f-b63aea1692c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Session_6",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
