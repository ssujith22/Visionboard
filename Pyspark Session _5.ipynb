{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83f57da3-6846-4f78-881d-aeb9493cf146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Problem  : Function like map() , filter() can use variables defined outside them in the driver program but each task running on the cluster gets a new copy of each variable, and updates from these copies are not propagated back to the driver.\n",
    "\n",
    "The Solution : Spark provides two type of shared variables.\n",
    "\n",
    "1.    Accumulators\n",
    "2.    Broadcast variables\n",
    "\n",
    "#####What is Broadcast Variable\n",
    "- they are immutable shared variable which are cached on each worker nodes on a Spark cluster.\n",
    "- Another Definition : Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.\n",
    "\n",
    "\n",
    "- Imagine you want to make some information, that information can be variable , rdd, collection of object, large datasets, databse connetion or anything, you want to make this information avaialable to all of your worker so that your executors can use that information & process that data as part of executing task, that’s process will complete by broadcast variable.\n",
    "\n",
    "#####Use Case\n",
    "- Let’s Imagine,. I have a large table of zip codes/pin code and want to perform the transformation on that data for analysis.\n",
    "\n",
    "Here, it is neither feasible to send the large lookup table every time to the executors, nor can we query the database every time. so, the solution should be to convert this lookup table to a broadcast variable and Spark will cache it in every executor for future reference.\n",
    "\n",
    "This will solve two main problems namely network overhead and time consumption\n",
    "\n",
    "- Broadcast Manager (BroadcastManager) is a Spark service to manage broadcast variables in Spark. It creates for a Spark application when SparkContext is initialized and is a simple wrapper around BroadcastFactory.\n",
    "\n",
    "- ContextCleaner is a Spark service that is responsible for application-wide cleanup of shuffles, RDDs, broadcasts, any many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9141044-569b-4efa-9b3a-81995f1900eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    }
   ],
   "source": [
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85e513ec-2707-4eae-afa6-99898dfb099f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Important  : When your work is complete with a broadcast variable, you should destroy it to release memory.\n",
    "\n",
    "broadcast.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18406067-dcac-483d-afb1-bc34ac992a4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n\n+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|James    |Smith   |USA    |CA   |\n|Michael  |Rose    |USA    |NY   |\n|Robert   |Williams|USA    |CA   |\n|Maria    |Jones   |USA    |FL   |\n+---------+--------+-------+-----+\n\n+---------+--------+-------+----------+\n|firstname|lastname|country|state     |\n+---------+--------+-------+----------+\n|James    |Smith   |USA    |California|\n|Michael  |Rose    |USA    |New York  |\n|Robert   |Williams|USA    |California|\n|Maria    |Jones   |USA    |Florida   |\n+---------+--------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "##BRoadcast example with dataframes\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9d4ac47-6190-4362-95a1-36746564e134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Accumulators provides a simple syntax for aggregating values from worker nodes back to the driver program.\n",
    "- Accumulators are write-only and initialize once variables where only tasks that are running on workers are allowed to update and updates from the workers get propagated automatically to the driver program. But, only the driver program is allowed to access the Accumulator variable using the value property.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0c6c9a3-9a99-40b7-bd35-2c4593e14bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n15\n5\n"
     ]
    }
   ],
   "source": [
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)\n",
    "\n",
    "accuSum=spark.sparkContext.accumulator(0)\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum+=x\n",
    "rdd.foreach(countFun)\n",
    "print(accuSum.value)\n",
    "\n",
    "accumCount=spark.sparkContext.accumulator(0)\n",
    "rdd2=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd2.foreach(lambda x:accumCount.add(1))\n",
    "print(accumCount.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ae71b7a-32c7-4f2a-acfd-5c36abdde4aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "####difference between client and cluster mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d14a953-9117-4874-b256-2b71d08f4921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Whenever we submit a Spark application to the cluster, the Driver or the Spark App Master should get started. And the Driver will be starting N number of workers. Spark driver will be managing spark context object to share the data and coordinates with the workers and cluster manager across the cluster. Cluster Manager can be Spark Standalone or Hadoop YARN or Mesos. Workers will be assigned a task and it will consolidate and collect the result back to the driver. A spark application gets executed within the cluster in two different modes – one is cluster mode and the second is client mode.\n",
    "\n",
    "#####Cluster Mode\n",
    "- In the cluster mode, the Spark driver or spark application master will get started in any of the worker machines. So, the client who is submitting the application can submit the application and the client can go away after initiating the application or can continue with some other work. So, it works with the concept of Fire and Forgets.\n",
    "\n",
    "##### Client Mode\n",
    "- In the client mode, the client who is submitting the spark application will start the driver and it will maintain the spark context. So, till the particular job execution gets over, the management of the task will be done by the driver. Also, the client should be in touch with the cluster. The client will have to be online until that particular job gets completed.\n",
    "\n",
    "- The default deployment mode is client mode.\n",
    "- In client mode, if a machine or a user session running spark-submit terminates, your application also terminates with status fail.\n",
    "- Using Ctrl-c after submitting the spark-submit command also terminates your application.\n",
    "- Client mode is not used for Production jobs. This is used for testing purposes.\n",
    "- Driver logs are accessible from the local machine itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6623333-c3ea-46cf-8f86-dd273a2486c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark-submit --deploy-mode client --driver-memory xxxx  ......\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a42e1c32-9c6d-4141-b8cd-63cbbe056afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Wide and Narrow Transformation\n",
    "- Narrow Transformation : Narrow transformations are the result of map() and filter() functions and these compute data that live on a single partition meaning there will not be any data movement between partitions to execute narrow transformations.\n",
    "- Functions such as map(), mapPartition(), flatMap(), filter(), union() are some examples of narrow transformation\n",
    "\n",
    "#####Wider Transformation\n",
    "- Wider transformations are the result of groupByKey() and reduceByKey() functions and these compute data that live on many partitions meaning there will be data movements between partitions to execute wider transformations. Since these shuffles the data, they also called shuffle transformations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd61b3e-3322-4be7-917d-86d8b0ac4459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s\nAlice’s Adventures in Wonderland\nProject Gutenberg’s\nAdventures in Wonderland\nProject Gutenberg’s\nProject\nGutenberg’s\nAlice’s\nAdventures\nin\nWonderland\nProject\nGutenberg’s\nAdventures\nin\nWonderland\nProject\nGutenberg’s\n"
     ]
    }
   ],
   "source": [
    "#flatmap example\n",
    "\n",
    "data = [\"Project Gutenberg’s\",\n",
    "        \"Alice’s Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "for element in rdd.collect():\n",
    "    print(element)\n",
    "\n",
    "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da0a4c6f-7bc1-461e-b583-846fb34cdf0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gutenberg’s', 3)\n('Adventures', 2)\n('Wonderland', 2)\n('Alice’s', 1)\n('in', 2)\n('Project', 3)\n"
     ]
    }
   ],
   "source": [
    "#reducebykey example\n",
    "data = [('Project', 1),\n",
    "('Gutenberg’s', 1),\n",
    "('Alice’s', 1),\n",
    "('Adventures', 1),\n",
    "('in', 1),\n",
    "('Wonderland', 1),\n",
    "('Project', 1),\n",
    "('Gutenberg’s', 1),\n",
    "('Adventures', 1),\n",
    "('in', 1),\n",
    "('Wonderland', 1),\n",
    "('Project', 1),\n",
    "('Gutenberg’s', 1)]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd2=rdd.reduceByKey(lambda a,b: a+b)\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4efc65c-4d07-42a9-8c4d-69bf9525dcda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####What is a UDF in Spark, and what are the challenges associated with using it?\n",
    "-  PySpark UDF’s are similar to UDF on traditional databases. In PySpark, you create a function in a Python syntax and wrap it with PySpark SQL udf() or register it as udf and use it on DataFrame and SQL respectively.\n",
    "\n",
    "#####Performance Association with UDF\n",
    "- Using UDFs can be an expensive process, as they may require data serialization and deserialization. Therefore, it’s important to use UDFs judiciously and only when built-in functions cannot meet your requirements.\n",
    "\n",
    "\n",
    "- Serialization and Deserialization Overhead:\n",
    " -  When using UDFs, the data needs to be serialized and deserialized between the JVM and the user-defined function this leads to significant performance overhead.\n",
    "- Garbage Collection:\n",
    "  - While using UDFs they can create temporary objects that can accumulate in the JVM heap, leading to garbage collection overhead.\n",
    "- Resource Utilization:\n",
    "  - UDFs can easily chew up loads of your resources, such as CPU and memory. So it’s necessary you carefully tune the Spark configuration parameters, such as spark.driver.memory, spark.executor.memory, spark.executor.cores, and spark.executor.instances.\n",
    "- Data Skew:\n",
    "  - UDFs can cause data skew, where some partitions have significantly more data than others. This can result in performance degradation and resource contention.\n",
    "\n",
    "  - Please practice more here : https://docs.databricks.com/en/udf/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724df4e5-0be9-4cc6-9716-67bbeb4f3d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n+-----+-------------+\n|Seqno|Name         |\n+-----+-------------+\n|1    |John Jones   |\n|2    |Tracey Smith |\n|3    |Amy Sanders  |\n+-----+-------------+\n\n+-----+------------+-------------+\n|Seqno|Name        |Cureated Name|\n+-----+------------+-------------+\n|1    |john jones  |JOHN JONES   |\n|2    |tracey smith|TRACEY SMITH |\n|3    |amy sanders |AMY SANDERS  |\n+-----+------------+-------------+\n\n+-----+-------------+\n|Seqno|Name         |\n+-----+-------------+\n|1    |John Jones   |\n|2    |Tracey Smith |\n|3    |Amy Sanders  |\n+-----+-------------+\n\n+-----+-----------+\n|Seqno|Name       |\n+-----+-----------+\n|1    |John Jones |\n+-----+-----------+\n\n+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n|4    |null        |\n+-----+------------+\n\n+------------------+\n|_nullsafeUDF(Name)|\n+------------------+\n|John Jones        |\n|Tracey Smith      |\n|Amy Sanders       |\n|                  |\n+------------------+\n\n+-----+-----------+\n|Seqno|Name       |\n+-----+-----------+\n|1    |John Jones |\n+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr \n",
    "\n",
    "\"\"\" Converting function to UDF \"\"\"\n",
    "convertUDF = udf(lambda z: convertCase(z))\n",
    "\n",
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    ".show(truncate=False)\n",
    "\n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "upperCaseUDF = udf(lambda z:upperCase(z),StringType())    \n",
    "\n",
    "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
    ".show(truncate=False)\n",
    "\n",
    "\"\"\" Using UDF on SQL \"\"\"\n",
    "spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
    "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\") \\\n",
    "     .show(truncate=False)\n",
    "     \n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE \" + \\\n",
    "          \"where Name is not null and convertUDF(Name) like '%John%'\") \\\n",
    "     .show(truncate=False)  \n",
    "     \n",
    "\"\"\" null check \"\"\"\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data,schema=columns)\n",
    "df2.show(truncate=False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
    "    \n",
    "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , StringType())\n",
    "\n",
    "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
    "          \" where Name is not null and _nullsafeUDF(Name) like '%John%'\") \\\n",
    "     .show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3940f046-5c09-4def-bec8-622e77c6ec42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####When should partitioning, bucketing, or both be used in Spark? \n",
    "\n",
    "- Partitioning in Spark\n",
    "\n",
    "  - Partitioning is a way to split data into separate folders on disk based on one or multiple columns. This enables efficient parallelism and partition pruning in Spark. Partition pruning is a technique used to optimize queries by skipping reading parts of the data that are not required.\n",
    "  - To partition a dataset, you need to provide the method with one or multiple columns to partition by. The dataset is then written to disk split by the partitioning column, with each partition saved into a separate folder on disk\n",
    "\n",
    "In Spark, partitioning is implemented by the .partitionBy() method of the DataFrameWriter class.\n",
    "\n",
    "- Bucketing in Spark\n",
    "\n",
    "  - Bucketing is a way to assign rows of a dataset to specific buckets and collocate them on disk. This enables efficient wide transformations in Spark, as the data is already collocated in the executors correctly. Wide transformations are operations that require shuffling data across partitions, which can be a costly operation.\n",
    "\n",
    "   - In Spark, bucketing is implemented by the .bucketBy() method of the DataFrameWriter class. To bucket a dataset, you need to provide the method with the number of buckets you want to create and the column to bucket by. The bucket number for a given row is assigned by calculating a hash on the bucket column and performing modulo by the number of desired buckets operation on the resulting hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d764c56-feb5-43ae-9656-98a0dd1f9364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"path/to/dataset\")\n",
    "\n",
    "#df.write.partitionBy(\"date\").format(\"parquet\").save(\"path/to/partitioned/dataset\")\n",
    "\n",
    "# Bucket the dataset by the \"id\" column into 10 buckets\n",
    "#df.write.bucketBy(10, \"id\").sortBy(\"id\").format(\"parquet\").save(\"path/to/bucketed/dataset\")\n",
    "# Bucketing is used when saving as a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f90f601-e465-474e-9f4e-c1d11ef12548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Session _5",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
